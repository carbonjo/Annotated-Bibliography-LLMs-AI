## Annotated Bibliography on LLMs and AI

### Foundational Papers

#### [Attention is All You Need](https://arxiv.org/abs/1706.03762)
- **Authors**: Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, ≈Å., & Polosukhin, I.
- **Publication Date**: 2017
- **Summary**: This paper introduces the Transformer model, a novel architecture that relies entirely on self-attention mechanisms to draw global dependencies between input and output.
- **Annotation**: The Transformer architecture has become foundational for modern LLMs, influencing models such as BERT and